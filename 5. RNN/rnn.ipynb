{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab462e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "134bc43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-23 16:25:24--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 228145 (223K) [text/plain]\n",
      "Saving to: ‘names.txt’\n",
      "\n",
      "names.txt           100%[===================>] 222.80K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2025-11-23 16:25:25 (11.4 MB/s) - ‘names.txt’ saved [228145/228145]\n",
      "\n",
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
    "words = open('./names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2958e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))  # unique chars in all names\n",
    "chars = ['.'] + chars                      # include start/end token\n",
    "\n",
    "stoi = {c:i for i,c in enumerate(chars)}\n",
    "itos = {i:c for c,i in stoi.items()}\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bdb0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle up the words\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "818808d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial hyperparameters\n",
    "vocab_size = len(itos)\n",
    "hidden_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50785774",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    xs = [stoi[c] for c in chs[:-1]]   # inputs\n",
    "    ys = [stoi[c] for c in chs[1:]]    # targets\n",
    "    dataset.append((xs, ys))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64f5411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0, 25, 21, 8, 5, 14, 7], [25, 21, 8, 5, 14, 7, 0]),\n",
       " ([0, 4, 9, 15, 14, 4, 18, 5], [4, 9, 15, 14, 4, 18, 5, 0]),\n",
       " ([0, 24, 1, 22, 9, 5, 14], [24, 1, 22, 9, 5, 14, 0])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3]  # three examples of (input, target) sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "893caac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: yuheng         .yuheng -> yuheng.\n",
      " 1: diondre        .diondre -> diondre.\n",
      " 2: xavien         .xavien -> xavien.\n",
      " 3: jori           .jori -> jori.\n",
      " 4: juanluis       .juanluis -> juanluis.\n",
      " 5: erandi         .erandi -> erandi.\n",
      " 6: phia           .phia -> phia.\n",
      " 7: samatha        .samatha -> samatha.\n"
     ]
    }
   ],
   "source": [
    "for i, (xs, ys) in enumerate(dataset[:8]):\n",
    "    xin = ''.join(itos[x] for x in xs)\n",
    "    yin = ''.join(itos[y] for y in ys)\n",
    "    print(f\"{i:2d}: {words[i]:12s}   {xin} -> {yin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "079feaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input indices for first name: tensor([ 0, 25, 21,  8,  5, 14,  7])\n",
      "Example target indices for first name: tensor([25, 21,  8,  5, 14,  7,  0])\n",
      "\n",
      "Example one-hot shapes for first name: torch.Size([27, 1]) torch.Size([27, 1])\n"
     ]
    }
   ],
   "source": [
    "# example one-hot vectors (first two time steps of first name)\n",
    "xs0 = torch.tensor(dataset[0][0], dtype=torch.long)\n",
    "print(\"Example input indices for first name:\", xs0)\n",
    "ys0 = torch.tensor(dataset[0][1], dtype=torch.long)\n",
    "print(\"Example target indices for first name:\", ys0)\n",
    "one_hot_t0 = F.one_hot(xs0[0], num_classes=vocab_size).float().unsqueeze(1)\n",
    "one_hot_t1 = F.one_hot(xs0[1], num_classes=vocab_size).float().unsqueeze(1)\n",
    "print(\"\\nExample one-hot shapes for first name:\", one_hot_t0.shape, one_hot_t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dda99e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_prev = torch.zeros((hidden_size, 1)) # initial hidden state\n",
    "Wxh = torch.randn((hidden_size, vocab_size), requires_grad=True) * 0.01 # weight input to hidden\n",
    "Whh = torch.randn((hidden_size, hidden_size), requires_grad=True) * 0.01 # weight hidden to hidden\n",
    "bh = torch.zeros((hidden_size, 1), requires_grad=True) # hidden bias\n",
    "Why = torch.randn((vocab_size, hidden_size), requires_grad=True) * 0.01 # weight hidden to output\n",
    "by = torch.zeros((vocab_size, 1), requires_grad=True) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f379d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Wxh @ one_hot_t0 + Whh @ h_prev + bh     # raw pre-activation\n",
    "h = torch.tanh(a)                     # hidden state\n",
    "y = Why @ h + by                     # logits\n",
    "probs = F.softmax(y, dim=0)          # probabilities\n",
    "\n",
    "# Wxh @ x0: meaning of the current character\n",
    "# Whh @ h_prev: memory of previous characters\n",
    "# tanh: bounded memory update\n",
    "# Why @ h: convert hidden state to next-char prediction\n",
    "# softmax: probability distribution over characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d169026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for entire na\n",
    "xs, ys = dataset[0]   # e.g. \".emma.\"\n",
    "T = len(xs)\n",
    "h_prev = torch.zeros(hidden_size, 1)\n",
    "loss = 0\n",
    "\n",
    "# ----- forward pass -----\n",
    "for t in range(T):\n",
    "    x_t = F.one_hot(torch.tensor(xs[t]), num_classes=vocab_size).float().unsqueeze(1) # one-hot vector 27 * 1 \n",
    "    a = Wxh @ x_t + Whh @ h_prev + bh # 32*27 @ 27*1 + 32*32 @ 32*1 + 32 * 1 = 32*1\n",
    "    h = torch.tanh(a) # 32*1\n",
    "    y = Why @ h + by # 27*32 @ 32*1 + 27*1 = 27*1\n",
    "    loss += F.cross_entropy(y.T, torch.tensor([ys[t]]))\n",
    "    h_prev = h # carry forward hidden state\n",
    "\n",
    "# ----- backward pass -----\n",
    "loss.backward()\n",
    "# PyTorch will compute:\n",
    "# dL/dWhy\n",
    "# dL/dWxh\n",
    "# dL/dWhh (most important part of BPTT)\n",
    "# dL/dbh\n",
    "# dL/dby\n",
    "# across ALL timesteps.\n",
    "# PyTorch keeps the entire chain linked.\n",
    "# When you call .backward(), it walks the computational graph from the final loss back through all timesteps automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2c232e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.0707, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc37303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random data scaled by 0.01 FIRST\n",
    "Wxh = (torch.randn((hidden_size, vocab_size)) * 0.01).to(device) # directly specifying requires_grad later, as required_grad=True gives a warning\n",
    "# in same variable raise error, as it is leaf variable and myltiplying by scalar makes it non-leaf\n",
    "Whh = (torch.randn((hidden_size, hidden_size)) * 0.01).to(device)\n",
    "bh = torch.zeros((hidden_size, 1)).to(device)\n",
    "Why = (torch.randn((vocab_size, hidden_size)) * 0.01).to(device)\n",
    "by = torch.zeros((vocab_size, 1)).to(device)\n",
    "\n",
    "# THEN tell PyTorch to track gradients for these leaves\n",
    "Wxh.requires_grad = True\n",
    "Whh.requires_grad = True\n",
    "bh.requires_grad = True\n",
    "Why.requires_grad = True\n",
    "by.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a45ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tensors = []\n",
    "for xs, ys in dataset:\n",
    "    xs_t = torch.tensor(xs, dtype=torch.long).to(device)\n",
    "    ys_t = torch.tensor(ys, dtype=torch.long).to(device)\n",
    "    dataset_tensors.append((xs_t, ys_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ccee5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d75a27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-273443584.py:37: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  p -= learning_rate * p.grad\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-273443584.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mp\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "params = [Wxh, Whh, bh, Why, by]\n",
    "V = vocab_size\n",
    "H = hidden_size\n",
    "\n",
    "for epoch in range(1_000):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for xs_t, ys_t in dataset_tensors:\n",
    "        for p in params:\n",
    "            p.grad = None\n",
    "\n",
    "        h_prev = torch.zeros(H, 1)\n",
    "        if torch.cuda.is_available():\n",
    "            h_prev = h_prev.cuda()\n",
    "\n",
    "        step_losses = []\n",
    "\n",
    "        for t in range(xs_t.size(0)):\n",
    "            x_t = F.one_hot(xs_t[t], num_classes=V).float().unsqueeze(1)\n",
    "            a = Wxh @ x_t + Whh @ h_prev + bh\n",
    "            h = torch.tanh(a)\n",
    "            y = Why @ h + by\n",
    "\n",
    "            step_loss = F.cross_entropy(y.T, ys_t[t].unsqueeze(0))\n",
    "            step_losses.append(step_loss)\n",
    "\n",
    "            h_prev = h\n",
    "\n",
    "        loss = torch.stack(step_losses).sum()\n",
    "        loss.backward()\n",
    "        # --- NEW: Clip gradients to prevent explosion ---\n",
    "        torch.nn.utils.clip_grad_norm_(params, max_norm=1.0) \n",
    "        # ------------------------------------------------\n",
    "        with torch.no_grad():\n",
    "            for p in params:\n",
    "                p -= learning_rate * p.grad\n",
    "\n",
    "        total_loss += float(loss)\n",
    "    if epoch % 100==0:\n",
    "        print(epoch, total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0615c987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generated Names ---\n",
      "anaitia\n",
      "jaa\n",
      "bmerim\n",
      "zamar\n",
      "zur\n",
      "goirie\n",
      "jilonnia\n",
      "alewia\n",
      "sovaria\n",
      "fayleigh\n"
     ]
    }
   ],
   "source": [
    "def sample_model(start_char='.', max_length=20):\n",
    "    # 1. Initialize input state\n",
    "    # Start with the hidden state at zero (like at the start of training)\n",
    "    h = torch.zeros((hidden_size, 1))\n",
    "    \n",
    "    # Start with the specific start_char (usually '.')\n",
    "    ix = stoi[start_char]\n",
    "    \n",
    "    output_name = []\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # --- Forward Pass (Same as training, but one step at a time) ---\n",
    "        \n",
    "        # Create input vector (one-hot) for the current character\n",
    "        x_t = F.one_hot(torch.tensor(ix), num_classes=vocab_size).float().unsqueeze(1)\n",
    "        \n",
    "        # Calculate hidden state update\n",
    "        h = torch.tanh(Wxh @ x_t + Whh @ h + bh)\n",
    "        \n",
    "        # Calculate logits (raw scores) for next character\n",
    "        logits = Why @ h + by\n",
    "        \n",
    "        # --- Sampling ---\n",
    "        \n",
    "        # Convert logits to probabilities (softmax)\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "        \n",
    "        # Sample the next character index from the distribution\n",
    "        # (This adds variety; taking the argmax would just repeat the same letters)\n",
    "        ix = torch.multinomial(probs.flatten(), num_samples=1).item()\n",
    "        \n",
    "        # Decode the index to a character\n",
    "        next_char = itos[ix]\n",
    "        \n",
    "        # Stop if we predict the end token\n",
    "        if next_char == '.':\n",
    "            break\n",
    "            \n",
    "        output_name.append(next_char)\n",
    "        \n",
    "    return ''.join(output_name)\n",
    "\n",
    "# Generate 10 names\n",
    "print(\"--- Generated Names ---\")\n",
    "for _ in range(10):\n",
    "    print(sample_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89874dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
